<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Meeting Room Robot – Architecture (Azure Search)</title>
  <style>
    :root {
      /* AD Ports–inspired palette: deep navy + maritime blue + teal */
      --bg: #020b1f;
      --card: #020b1f;
      --border: #16324f;
      --accent: #00b0ca;   /* teal / aqua */
      --accent-2: #004a7c; /* deep maritime blue */
      --accent-3: #5cd4ff; /* light cyan accent */
      --accent-4: #f9a826; /* warm amber */
      --text-main: #e5f4ff;
      --text-muted: #9fb3d1;
      --text-soft: #7b8ba8;
      --shadow-soft: 0 20px 55px rgba(3, 12, 30, 0.95);
      --radius-lg: 18px;
      --radius-xl: 22px;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      padding: 24px;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
        "Segoe UI", sans-serif;
      background: radial-gradient(circle at top, #05142e 0, #020b1f 40%, #000814 100%);
      color: var(--text-main);
    }

    .page {
      max-width: 1200px;
      margin: 0 auto;
    }

    .title {
      font-size: 24px;
      font-weight: 650;
      letter-spacing: 0.16em;
      margin-bottom: 10px;
      display: flex;
      align-items: center;
      gap: 10px;
      text-transform: uppercase;
      color: #e6f5ff;
    }

    .title span.robot-dot {
      width: 14px;
      height: 14px;
      border-radius: 999px;
      background: radial-gradient(circle, #b9f2ff 0, var(--accent) 40%, #00a3b8 100%);
      box-shadow: 0 0 18px rgba(0, 176, 202, 0.9);
      border: 1px solid rgba(92, 212, 255, 0.9);
    }

    .subtitle {
      font-size: 12px;
      color: var(--text-muted);
      max-width: 760px;
      margin-bottom: 18px;
    }

    .legend {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin-bottom: 22px;
      font-size: 10px;
      color: var(--text-soft);
    }

    .legend-item {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 5px 10px;
      border-radius: 999px;
      background: radial-gradient(circle at top left, rgba(0, 74, 124, 0.9), #020b1f);
      border: 1px solid rgba(29, 64, 105, 0.95);
    }

    .legend-dot {
      width: 9px;
      height: 9px;
      border-radius: 999px;
    }

    .legend-dot.browser {
      background: linear-gradient(135deg, var(--accent), #5cd4ff);
    }
    .legend-dot.backend {
      background: linear-gradient(135deg, var(--accent-2), #00b0ca);
    }
    .legend-dot.data {
      background: linear-gradient(135deg, #6d28d9, #a855f7);
    }
    .legend-dot.external {
      background: linear-gradient(135deg, var(--accent-4), #fbbf24);
    }

    .section {
      border-radius: 24px;
      padding: 18px 18px 16px;
      margin-bottom: 16px;
      background: radial-gradient(circle at top left, #06152d 0, #020b1f 55%, #020b1f 100%);
      border: 1px solid rgba(22, 50, 79, 0.95);
      box-shadow: var(--shadow-soft);
    }

    .section-header {
      display: flex;
      align-items: baseline;
      justify-content: space-between;
      gap: 10px;
      margin-bottom: 14px;
    }

    .section-title {
      font-size: 13px;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      color: var(--text-muted);
    }

    .section-tag {
      font-size: 10px;
      padding: 4px 9px;
      border-radius: 999px;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      background: linear-gradient(135deg, rgba(0, 74, 124, 0.88), rgba(3, 18, 39, 0.98));
      color: #c0e4ff;
      border: 1px solid rgba(61, 110, 153, 0.95);
    }

    .section-desc {
      font-size: 11px;
      color: var(--text-soft);
      margin-bottom: 12px;
      max-width: 880px;
    }

    /* Flow rows */
    .flow-row {
      display: flex;
      align-items: stretch;
      justify-content: center;
      gap: 12px;
      margin-bottom: 10px;
      flex-wrap: wrap;
    }

    .node {
      min-width: 170px;
      max-width: 260px;
      border-radius: var(--radius-lg);
      padding: 10px 11px;
      font-size: 11px;
      border: 1px solid rgba(31, 64, 101, 0.95);
      background: radial-gradient(circle at top left, rgba(8, 47, 73, 0.9), #020b1f);
      position: relative;
    }

    .node-title {
      font-size: 11px;
      font-weight: 600;
      margin-bottom: 4px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 4px;
      color: #e6f5ff;
    }

    .node-title span.badge {
      font-size: 9px;
      padding: 2px 6px;
      border-radius: 999px;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      background: rgba(4, 25, 50, 0.98);
      border: 1px solid rgba(63, 103, 140, 0.95);
      color: var(--text-soft);
    }

    .node-body {
      color: var(--text-muted);
      line-height: 1.4;
    }

    .node-body ul {
      padding-left: 12px;
      margin: 3px 0 0;
    }

    .node-body li {
      margin-bottom: 2px;
    }

    /* Node themes */
    .node.browser {
      border-color: rgba(0, 176, 202, 0.9);
      background: radial-gradient(circle at top left, rgba(0, 176, 202, 0.18), #020b1f);
    }

    .node.backend {
      border-color: rgba(0, 114, 206, 0.9);
      background: radial-gradient(circle at top left, rgba(0, 114, 206, 0.22), #020b1f);
    }

    .node.data {
      border-color: rgba(129, 140, 248, 0.95);
      background: radial-gradient(circle at top left, rgba(79, 70, 229, 0.18), #020b1f);
    }

    .node.external {
      border-color: rgba(249, 168, 37, 0.95);
      background: radial-gradient(circle at top left, rgba(249, 168, 37, 0.16), #020b1f);
    }

    .node.robot {
      border-color: rgba(0, 176, 202, 1);
      background: radial-gradient(circle at top left, rgba(0, 176, 202, 0.24), #020b1f);
    }

    /* Arrows */
    .arrow {
      display: flex;
      align-items: center;
      justify-content: center;
      min-width: 80px;
      padding: 4px 8px;
      border-radius: 999px;
      border: 1px dashed rgba(100, 116, 139, 0.9);
      color: var(--text-soft);
      font-size: 10px;
      position: relative;
      white-space: nowrap;
      background: rgba(4, 25, 50, 0.9);
    }

    .arrow::after {
      content: "➜";
      font-size: 13px;
      margin-left: 6px;
      color: var(--accent-3);
    }

    .arrow.vertical {
      flex-direction: column;
      width: auto;
      padding: 2px 6px;
    }

    .arrow.vertical::after {
      content: "⬇";
      font-size: 12px;
      margin: 2px 0 0 0;
    }

    .arrow.highlight {
      border-color: rgba(0, 176, 202, 0.95);
      color: #c0e4ff;
      background: radial-gradient(circle at center, rgba(0, 176, 202, 0.18), #020b1f);
    }

    /* Summary strip */
    .footer-strip {
      margin-top: 16px;
      font-size: 11px;
      color: var(--text-soft);
      padding: 10px 12px;
      border-radius: 999px;
      border: 1px solid rgba(31, 64, 101, 0.95);
      background: radial-gradient(circle at left, rgba(0, 176, 202, 0.18), #020b1f);
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      align-items: center;
    }

    .footer-strip strong {
      color: var(--accent-3);
      font-weight: 600;
    }

    @media (max-width: 900px) {
      .flow-row {
        justify-content: flex-start;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <div class="title">
      <span class="robot-dot"></span>
      Meeting Room Robot – Architecture with Azure AI Search
    </div>
    <div class="subtitle">
      Browser-based robot page streams audio to an n8n Audio Gateway, uses 11Labs for diarized
      transcription, stores chunks as documents in Azure AI Search for semantic memory, and uses
      an n8n Robot Brain to raise human-gated robot signals before speaking back with full context.
    </div>

    <div class="legend">
      <div class="legend-item">
        <span class="legend-dot browser"></span> Browser / Meeting Room UI
      </div>
      <div class="legend-item">
        <span class="legend-dot backend"></span> Backend & APIs / n8n Workflows
      </div>
      <div class="legend-item">
        <span class="legend-dot.data"></span> Azure AI Search & Metadata DB
      </div>
      <div class="legend-item">
        <span class="legend-dot.external"></span> External AI Services (11Labs, Azure OpenAI, etc.)
      </div>
    </div>

    <!-- SECTION 1: Real-time listening & transcription -->
    <div class="section">
      <div class="section-header">
        <div class="section-title">1. Real-Time Listening & Transcription</div>
        <div class="section-tag">Data Plane – Audio & STT</div>
      </div>
      <div class="section-desc">
        Browser mic sends audio into an n8n-based Audio Gateway / STT Proxy, which calls 11Labs
        for diarized transcription and passes normalized text segments into the chunking and indexing
        pipeline.
      </div>

      <div class="flow-row">
        <div class="node browser">
          <div class="node-title">
            <span>Robot Portal (Browser)</span>
            <span class="badge">Client</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Mic capture (getUserMedia + MediaRecorder)</li>
              <li>Uploads audio chunks (<code>/audio-chunk</code>) with meeting_id</li>
              <li>Shows live transcript + robot suggestions UI</li>
            </ul>
          </div>
        </div>

        <div class="arrow">audio chunks (webm/pcm)</div>

        <div class="node backend">
          <div class="node-title">
            <span>n8n – Audio Gateway / STT Proxy</span>
            <span class="badge">Workflow A</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Webhook node: receive audio chunk</li>
              <li>Function: prepare STT request (meeting_id, diarization)</li>
              <li>Ensures idempotent <code>client_chunk_id</code></li>
            </ul>
          </div>
        </div>

        <div class="arrow">STT call</div>

        <div class="node external">
          <div class="node-title">
            <span>11Labs STT + Diarization</span>
            <span class="badge">External</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Multi-speaker diarization enabled</li>
              <li>Returns segments + timestamps per speaker</li>
              <li>Partial & final hypotheses</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="flow-row">
        <div class="arrow highlight vertical">
          final diarized text
        </div>

        <div class="node backend">
          <div class="node-title">
            <span>n8n – Normalize & Chunk Builder</span>
            <span class="badge">Workflow A</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Normalize 11Labs output to clean transcript</li>
              <li>Aggregate into 5–10 sec chunks per meeting</li>
              <li>Compute <code>chunkIndex</code>, <code>timestampStart/End</code></li>
            </ul>
          </div>
        </div>

        <div class="arrow">chunk doc (JSON)</div>

        <div class="node external">
          <div class="node-title">
            <span>Azure OpenAI – Embeddings</span>
            <span class="badge">External</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Input: chunk <code>text</code> content</li>
              <li>Output: embedding vector (e.g. 1536 dims)</li>
              <li>Attached as <code>textVector</code> field</li>
            </ul>
          </div>
        </div>

        <div class="arrow">doc + vector</div>

        <div class="node data">
          <div class="node-title">
            <span>Azure AI Search – Index: <code>meeting-transcript-chunks</code></span>
            <span class="badge">Memory</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Document per chunk (meetingId, chunkIndex, timestamps)</li>
              <li>Fields: <code>text</code>, <code>speakerLabels</code>, metadata</li>
              <li>Vector field: <code>textVector</code> for semantic search</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- SECTION 2: Storage & semantic context (Azure Search + DB) -->
    <div class="section">
      <div class="section-header">
        <div class="section-title">2. Memory & Semantic Context (Azure Search + Metadata DB)</div>
        <div class="section-tag">Context Layer</div>
      </div>
      <div class="section-desc">
        Azure AI Search becomes the canonical memory for all diarized transcript chunks. A small
        metadata DB tracks meetings, robot signals, and robot turns, while background workflows
        maintain semantic state (decisions, risks, actions) on top of the search index.
      </div>

      <div class="flow-row">
        <div class="node data">
          <div class="node-title">
            <span>Metadata DB – <code>meeting_sessions</code></span>
            <span class="badge">Relational</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Meeting metadata (title, tenant, host)</li>
              <li>Start / end timestamps</li>
              <li>Links to robot signals & robot turns</li>
            </ul>
          </div>
        </div>

        <div class="arrow vertical">
          1:N relationship via meetingId
        </div>

        <div class="node data">
          <div class="node-title">
            <span>Azure AI Search – Transcript Index</span>
            <span class="badge">Search</span>
          </div>
          <div class="node-body">
            <ul>
              <li>All chunks for all meetings as documents</li>
              <li>Filterable by <code>meetingId</code>, <code>tenantId</code>, times</li>
              <li>Supports vector + semantic + keyword queries</li>
            </ul>
          </div>
        </div>

        <div class="arrow vertical">
          background enrichment
        </div>

        <div class="node external">
          <div class="node-title">
            <span>n8n – Semantic State Builder</span>
            <span class="badge">Workflow</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Polls new chunks or uses Azure Search queries</li>
              <li>LLM extracts decisions, actions, risks</li>
              <li>Writes structured state into metadata DB (e.g. <code>meeting_semantic_state</code>)</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="flow-row">
        <div class="node data">
          <div class="node-title">
            <span>Metadata DB – <code>robot_signals</code></span>
            <span class="badge">Control</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Pending / accepted / rejected / expired signals</li>
              <li>Linked to meeting via <code>meetingId</code></li>
              <li>Used for human-gated turn-taking</li>
            </ul>
          </div>
        </div>

        <div class="arrow vertical">
          1:N per meeting
        </div>

        <div class="node data">
          <div class="node-title">
            <span>Metadata DB – <code>robot_turns</code></span>
            <span class="badge">Memory</span>
          </div>
          <div class="node-body">
            <ul>
              <li>What robot actually said (utterances)</li>
              <li>Associated signal, time, feedback</li>
              <li>Helps avoid repetition and tune behaviour</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- SECTION 3: Robot signals & responses (Robot Brain) -->
    <div class="section">
      <div class="section-header">
        <div class="section-title">3. Robot Brain – Signals & Human-Gated Responses</div>
        <div class="section-tag">Decision & Control Plane</div>
      </div>
      <div class="section-desc">
        An n8n-based Robot Brain listens to transcript events, uses Azure AI Search to fetch recent
        context and semantically relevant chunks, consults its own memory of previous actions, and
        decides when to raise a new robot signal. Only the last accepted signal drives what the robot
        will actually say.
      </div>

      <div class="flow-row">
        <div class="node backend">
          <div class="node-title">
            <span>n8n – Transcript Event Trigger</span>
            <span class="badge">Workflow B</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Triggered when new chunk indexed or event emitted</li>
              <li>Receives <code>meetingId</code>, <code>chunkIndex</code></li>
              <li>Function: throttle evaluations per meeting</li>
            </ul>
          </div>
        </div>

        <div class="arrow">context fetch</div>

        <div class="node data">
          <div class="node-title">
            <span>Azure AI Search – Context Queries</span>
            <span class="badge">Search</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Fetch last N chunks: filter by <code>meetingId</code>, sort by time</li>
              <li>Semantic / vector queries: “decisions”, “risks”, “open questions”</li>
              <li>Builds a compact recent context window for the LLM</li>
            </ul>
          </div>
        </div>

        <div class="arrow">semantic + memory</div>

        <div class="node data">
          <div class="node-title">
            <span>Metadata DB – Semantic State & Robot Memory</span>
            <span class="badge">DB</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Read structured state from <code>meeting_semantic_state</code></li>
              <li>Read prior <code>robot_signals</code> + <code>robot_turns</code></li>
              <li>Include feedback to avoid repeated suggestions</li>
            </ul>
          </div>
        </div>

        <div class="arrow">LLM reasoning</div>

        <div class="node external">
          <div class="node-title">
            <span>Azure OpenAI – Decision LLM</span>
            <span class="badge">External</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Input: recent transcript + semantic state + memory</li>
              <li>Output: potential issues, questions, missing owners/dates</li>
              <li>n8n Function interprets to decide <code>raise_signal</code> & type</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="flow-row">
        <div class="arrow vertical">
          branch: raise or skip
        </div>

        <div class="node external">
          <div class="node-title">
            <span>Azure OpenAI – Draft Signal Content</span>
            <span class="badge">External</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Generates: <strong>title</strong>, <strong>preview</strong>, <strong>full_message</strong></li>
              <li>Signal types: clarification, summary, risk, action items, etc.</li>
              <li>Short, human-friendly explanation for the host</li>
            </ul>
          </div>
        </div>

        <div class="arrow highlight">create robot signal</div>

        <div class="node backend">
          <div class="node-title">
            <span>Backend API – <code>POST /meetings/{id}/robot/signals</code></span>
            <span class="badge">API</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Writes signal to <code>robot_signals</code> (metadata DB)</li>
              <li>Assigns <code>signalIndex</code>, sets state = pending</li>
              <li>Publishes event to browser clients via WS/SSE</li>
            </ul>
          </div>
        </div>

        <div class="arrow">WS event</div>

        <div class="node robot">
          <div class="node-title">
            <span>Robot Suggestions Panel (Browser)</span>
            <span class="badge">UI</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Displays pending signals (title + preview + type)</li>
              <li>Host clicks Accept / Dismiss</li>
              <li>Only last accepted signal is considered “active”</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="flow-row">
        <div class="node browser">
          <div class="node-title">
            <span>Host Accepts Signal</span>
            <span class="badge">Browser</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Calls <code>POST /robot-signals/{id}/accept</code></li>
              <li>Optionally includes user/device info</li>
            </ul>
          </div>
        </div>

        <div class="arrow highlight">
          mark last accepted
        </div>

        <div class="node backend">
          <div class="node-title">
            <span>Backend – Signal State Manager</span>
            <span class="badge">API</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Marks previous accepted signals as expired</li>
              <li>Marks current signal as accepted</li>
              <li>Emits “signal_accepted” event for Robot Brain</li>
            </ul>
          </div>
        </div>

        <div class="arrow">context + signal</div>

        <div class="node external">
          <div class="node-title">
            <span>Azure OpenAI + 11Labs – Robot Response</span>
            <span class="badge">External</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Fetches broader context from Azure Search (full meeting)</li>
              <li>LLM crafts final robot response with full awareness</li>
              <li>11Labs TTS generates speech to play into room</li>
            </ul>
          </div>
        </div>

        <div class="arrow vertical">
          log & learn
        </div>

        <div class="node data">
          <div class="node-title">
            <span>Metadata DB – Log Robot Utterance</span>
            <span class="badge">Memory</span>
          </div>
          <div class="node-body">
            <ul>
              <li>Store spoken text, timing, related signal</li>
              <li>Feedback: helpful / not helpful (optional)</li>
              <li>Feeds future Robot Brain decisions & analytics</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="footer-strip">
      <strong>Concept in one line:</strong>
      Browser mic → n8n Audio Gateway → 11Labs STT → Azure AI Search transcript index →
      n8n Robot Brain + Azure OpenAI → human-approved robot signal → robot speaks back with
      full, Azure Search–powered context.
    </div>
  </div>
</body>
</html>
